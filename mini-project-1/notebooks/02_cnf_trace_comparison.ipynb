{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e304d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scikit-learn\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8505f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7479b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona a pasta mini_project_1 como root\n",
    "sys.path.append(os.path.abspath('..')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.cnf import CNF\n",
    "from src.models.vector_field import VectorField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de9264",
   "metadata": {},
   "source": [
    "# Milestone 2: Continuous Normalizing Flow (CNF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7f724",
   "metadata": {},
   "source": [
    "## Fundamentação Teórica \n",
    "\n",
    "### Mudança de variável\n",
    "\n",
    "Para uma transformação invertível $z=f(x)$:\n",
    "\n",
    "$$logp(x) = log p (z) + log \\Bigl |det \\frac{\\partial f}{\\partial x} \\Bigr|$$\n",
    "\n",
    "Para CNF com $\\phi_t(x)$ integrando $\\frac{dx}{dt} = f(x,t)$:\n",
    "\n",
    "$$logp(x) = log p (z) + \\int_{0}^{1}tr \\Bigl( \\frac{\\partial f}{\\partial x} \\Bigr) dt$$\n",
    "\n",
    "onde $z=\\phi_0(x)$ (integração reversa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7e782",
   "metadata": {},
   "source": [
    "### Propriedades da Equação de Divergência:\n",
    "Para calcular a variação da log-probabilidade em um CNF, utilizamos a divergência:\n",
    " $$\\text{div}(f) = \\text{tr}\\left( \\frac{\\partial f}{\\partial x} \\right)$$\n",
    "\n",
    "* $f$: Campo de vetores (Vector Field) definido pela rede neural, onde $f: \\mathbb{R}^d \\to \\mathbb{R}^d$.\n",
    "* $x$: Posição no espaço de estados, $x \\in \\mathbb{R}^d$.\n",
    "* $\\frac{\\partial f}{\\partial x}$: Matriz Jacobiana de $f$ em relação a $x$, de dimensão $d \\times d$.\n",
    "* $\\text{tr}(\\cdot)$: Operador traço, que soma os elementos da diagonal principal da matriz Jacobiana.\n",
    "* Complexidade: $O(d^2)$, já que requer $d$ passagens de autograd para extrair cada componente da diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bcef",
   "metadata": {},
   "source": [
    "## Treino com Moons e Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd3daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.datasets import make_moons, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d16055",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "func must be an instance of nn.Module to specify the adjoint parameters; alternatively they can be specified explicitly via the `adjoint_params` argument. If there are no parameters then it is allowable to set `adjoint_params=()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m501\u001b[39m):\n\u001b[32m     17\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     loss = -\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m.mean()\n\u001b[32m     19\u001b[39m     loss.backward()\n\u001b[32m     20\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\loren\\Projetos_Antigos\\flow\\mini-project-1\\src\\models\\cnf.py:119\u001b[39m, in \u001b[36mCNF.log_prob\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03mCalcula log p(x) usando change of variables.\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Forward pass: x → z\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m z, delta_log_det = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# log p(z) do prior\u001b[39;00m\n\u001b[32m    122\u001b[39m log_pz = \u001b[38;5;28mself\u001b[39m.base_dist.log_prob(z)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\loren\\Projetos_Antigos\\flow\\mini-project-1\\src\\models\\cnf.py:100\u001b[39m, in \u001b[36mCNF.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Integrar\u001b[39;00m\n\u001b[32m     99\u001b[39m t_span = torch.tensor([\u001b[32m0.\u001b[39m, \u001b[32m1.\u001b[39m]).to(x)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m state_1 = \u001b[43modeint_adjoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_augmented_dynamics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdopri5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[-\u001b[32m1\u001b[39m] \u001b[38;5;66;03m# Pegar apenas t=1\u001b[39;00m\n\u001b[32m    109\u001b[39m z = state_1[:, :-\u001b[32m1\u001b[39m]\n\u001b[32m    110\u001b[39m delta_log_det = state_1[:, -\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\loren\\Projetos_Antigos\\flow\\.venv\\Lib\\site-packages\\torchdiffeq\\_impl\\adjoint.py:162\u001b[39m, in \u001b[36modeint_adjoint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34modeint_adjoint\u001b[39m(func, y0, t, *, rtol=\u001b[32m1e-7\u001b[39m, atol=\u001b[32m1e-9\u001b[39m, method=\u001b[38;5;28;01mNone\u001b[39;00m, options=\u001b[38;5;28;01mNone\u001b[39;00m, event_fn=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    157\u001b[39m                    adjoint_rtol=\u001b[38;5;28;01mNone\u001b[39;00m, adjoint_atol=\u001b[38;5;28;01mNone\u001b[39;00m, adjoint_method=\u001b[38;5;28;01mNone\u001b[39;00m, adjoint_options=\u001b[38;5;28;01mNone\u001b[39;00m, adjoint_params=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    158\u001b[39m \n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# We need this in order to access the variables inside this module,\u001b[39;00m\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# since we have no other way of getting variables along the execution path.\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m adjoint_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, nn.Module):\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mfunc must be an instance of nn.Module to specify the adjoint parameters; alternatively they \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    163\u001b[39m                          \u001b[33m'\u001b[39m\u001b[33mcan be specified explicitly via the `adjoint_params` argument. If there are no parameters \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    164\u001b[39m                          \u001b[33m'\u001b[39m\u001b[33mthen it is allowable to set `adjoint_params=()`.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Must come before _check_inputs as we don't want to use normalised input (in particular any changes to options)\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m adjoint_rtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: func must be an instance of nn.Module to specify the adjoint parameters; alternatively they can be specified explicitly via the `adjoint_params` argument. If there are no parameters then it is allowable to set `adjoint_params=()`."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dados\n",
    "X, _ = make_moons(n_samples=2000, noise=0.05)\n",
    "X = torch.tensor(X).float().to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "# 2. Instanciação do Modelo e Otimizador\n",
    "# 'model' é a instância de CNF definida anteriormente\n",
    "model = CNF(VectorField(features=2)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(501):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -model.log_prob(X).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "\n",
    "def plot_density(model):\n",
    "    with torch.no_grad():\n",
    "        x = np.linspace(-2, 3, 100)\n",
    "        y = np.linspace(-1, 2, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        grid = torch.tensor(np.stack([X.flatten(), Y.flatten()], axis=1)).float().to(device)\n",
    "        log_prob = model.log_prob(grid).cpu().numpy()\n",
    "        Z = np.exp(log_prob).reshape(100, 100)\n",
    "        plt.contourf(X, Y, Z, levels=50)\n",
    "        plt.title(\"Densidade Aprendida pelo CNF (Traço Exato)\")\n",
    "        plt.show()\n",
    "\n",
    "plot_density(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70d506",
   "metadata": {},
   "source": [
    "## Treino com MNIST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
